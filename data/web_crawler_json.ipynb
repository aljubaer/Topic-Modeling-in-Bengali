{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36\"}\n",
    "headers = {}\n",
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_prothom_alo_into_jsob(start_date_str, end_date_str):\n",
    "    from datetime import datetime, timedelta\n",
    "    base_url = \"https://www.prothomalo.com\"\n",
    "    archive_url = base_url+\"/archive/\"\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "    delta = end_date - start_date\n",
    "    j = -1\n",
    "    per_data = {}\n",
    "    for i in range(delta.days + 1):\n",
    "        date = start_date + timedelta(days = i)\n",
    "        url = archive_url+date.strftime(\"%Y-%m-%d\")\n",
    "        infile = open('newspaper/prothom_alo_'+ date.strftime(\"%Y-%m-%d\") + \".txt\", \"w+\", encoding = 'utf-8')\n",
    "        while url:\n",
    "            j = j + 1\n",
    "            print(\"reading url >>> \"+url)\n",
    "            try:\n",
    "                html_content = requests.get(url, headers = headers).content\n",
    "            except:\n",
    "                print('Connection Problem for url: ', url, '. Skipping it.')\n",
    "                break\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "            links = soup.findAll('a', {'class': 'link_overlay'}, href = True)\n",
    "            for link in links:\n",
    "                try:\n",
    "                    inner_html_content = requests.get(base_url+link['href'], headers = headers).content\n",
    "                except:\n",
    "                    print('Connection Problem for article, url: ', base_url+link['href'])\n",
    "                    continue\n",
    "                inner_soup = BeautifulSoup(inner_html_content, \"lxml\")\n",
    "                articleBody = inner_soup.find('div', {'itemprop': 'articleBody'})\n",
    "                if articleBody:\n",
    "                    headline = inner_soup.find('div', {'class': 'right_title'}).text\n",
    "#                     print(\"headline >>> \"+headline)\n",
    "                    per_data[str(j)] = headline + \" -> \" + str(articleBody.text)\n",
    "#                     print(per_data[\"headline\"])\n",
    "#                     per_data[\"article\"] = str(articleBody.text)\n",
    "#                     infile.write(headline+\"\\n\")\n",
    "#                     infile.write(str(articleBody.text) + '\\n\\n')\n",
    "#                     data.append(per_data)\n",
    "\t\t\t\n",
    "            pagination_div = soup.find('div', {'class': 'pagination'})\n",
    "            url = None\n",
    "            if pagination_div:\n",
    "                next_page_link = pagination_div.find('a', {'class': 'next_page'}, href = True)\n",
    "                if next_page_link:\n",
    "                    url = next_page_link['href']\n",
    "        \n",
    "        data[\"content\"] = per_data\n",
    "#         infile.write(str(data))\n",
    "        json.dump(data, infile)\n",
    "        infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading url >>> https://www.prothomalo.com/archive/2019-09-02\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-02?page=2\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-02?page=3\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-02?page=4\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-02?page=5\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-02?page=6\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-02?page=7\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-02?page=8\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-03\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-03?page=2\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-03?page=3\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-03?page=4\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-03?page=5\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-03?page=6\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-03?page=7\n",
      "reading url >>> https://www.prothomalo.com/archive/2019-09-03?page=8\n"
     ]
    }
   ],
   "source": [
    "crawl_prothom_alo_into_jsob(\"2019-09-02\",\"2019-09-03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
