{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(path):\n",
    "    files = []\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.txt' in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "\n",
    "    all_data_json_list = []\n",
    "    for f in files:\n",
    "        with open(f, encoding = 'utf-8') as json_file:\n",
    "            sdata = json.load(json_file)\n",
    "            for data in sdata:\n",
    "                all_data_json_list.append(data)\n",
    "    print('Total data read -> ' + str(len(all_data_json_list)))\n",
    "    return all_data_json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToDataFrame(data_json):\n",
    "    df = pd.DataFrame(data_json)\n",
    "    print('DataFrame shape' + str(df.shape))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToList(data_json_list, key):\n",
    "    data_list = []\n",
    "    for single_data_json in data_json_list:\n",
    "        data_list.append(single_data_json[key])\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_bengali_letters(char):\n",
    "    return ord(char) >= 2433 and ord(char) <= 2543 \n",
    "\n",
    "def get_replacement(char):\n",
    "    if valid_bengali_letters(char):\n",
    "        return char\n",
    "    newlines = [10, 2404, 2405, 2551, 9576]\n",
    "    if ord(char) in newlines: \n",
    "        return ' '\n",
    "    return ' ';\n",
    "\n",
    "def get_valid_lines(line):\n",
    "    copy_line = ''\n",
    "    for letter in line:\n",
    "        copy_line += get_replacement(letter)\n",
    "    return copy_line\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(nltk.word_tokenize(get_valid_lines(sentence)))  # deacc=True removes punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_file = open('stop_words.txt', \"r+\", encoding = 'utf-8')\n",
    "all_stopwords = stopwords_file.read()\n",
    "stopwords_ready = [word.strip() for word in all_stopwords.split()]\n",
    "def remove_stopwords(content):   \n",
    "    without_stopwords = []\n",
    "    for word in content:\n",
    "        if word not in stopwords_ready and len(word) > 5:\n",
    "            without_stopwords.append(word)\n",
    "    return without_stopwords\n",
    "def remove_stopwords_list(data_list):\n",
    "    data_without_stopwords_list = []\n",
    "    for content in data_list:\n",
    "        data_without_stopwords_list.append(remove_stopwords(content))\n",
    "    return data_without_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from b_parser import RafiStemmer\n",
    "stemmer = RafiStemmer()\n",
    "def stemming_data(content):\n",
    "    for i, word in enumerate(content):\n",
    "        content[i] = stemmer.stem_word(word)\n",
    "    return content\n",
    "def stemming_data_list(data_list):\n",
    "    for i, sdata in enumerate(data_list):\n",
    "        data_list[i] = stemming_data(sdata)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLda(data_ready, num_topics = 10, iterations = 1000, alpha='auto'):\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "    # Create Corpus: Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=num_topics, \n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=30,\n",
    "                                               passes=30,\n",
    "                                               alpha=alpha,\n",
    "                                               iterations=iterations,\n",
    "                                               per_word_topics=True)\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldaOutputProducer(lda_model):\n",
    "    x = (lda_model.show_topics(num_topics=20, num_words=40,formatted=False))\n",
    "    topics_words = [(tp[0], [wd[0] for wd in tp[1]], [wd[1] for wd in tp[1]]) for tp in x]\n",
    "    output_json_list = []\n",
    "    for topic,words,conts in topics_words:\n",
    "        topic_json = {}\n",
    "        topic_content = {}\n",
    "        topic_content[\"words\"] = words\n",
    "        topic_content[\"conts\"] = conts\n",
    "        #topic_json[str(topic)] = topic_content\n",
    "        output_json_list.append(topic_content)\n",
    "    out_df = convertToDataFrame(output_json_list)\n",
    "    out_df.to_json(r'topic_dist.txt')\n",
    "    #write = open('topic_output.txt', 'w+', encoding='utf-8')\n",
    "    #json.dump(out, write, indent=2, ensure_ascii=False)\n",
    "    #write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data read -> 812\n"
     ]
    }
   ],
   "source": [
    "data_json_list = readData('/home/aljubaer/Desktop/1_spl_3/data/newspaper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape(812, 3)\n"
     ]
    }
   ],
   "source": [
    "data_df = convertToDataFrame(data_json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = convertToList(data_json_list, 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenized_list = list(sent_to_words(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_stopwords_list = remove_stopwords_list(data_tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stemmed_list = stemming_data_list(data_without_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = runLda(data_stemmed_list, num_topics = 10, iterations = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape(10, 2)\n"
     ]
    }
   ],
   "source": [
    "ldaOutputProducer(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
