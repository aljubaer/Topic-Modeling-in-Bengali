{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36\"}\n",
    "headers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_prothom_alo(start_date_str, end_date_str):\n",
    "    from datetime import datetime, timedelta\n",
    "    base_url = \"https://www.prothomalo.com\"\n",
    "    archive_url = base_url+\"/archive/\"\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "    delta = end_date - start_date\n",
    "\t\n",
    "    for i in range(delta.days + 1):\n",
    "\n",
    "        date = start_date + timedelta(days = i)\n",
    "        url = archive_url+date.strftime(\"%Y-%m-%d\")\n",
    "        infile = open('newspaper/prothom_alo_'+ date.strftime(\"%Y-%m-%d\") + \".txt\", \"w+\", encoding = 'utf-8')\n",
    "        while url:\n",
    "            print(\"reading url >>> \"+url)\n",
    "            try:\n",
    "                html_content = requests.get(url, headers = headers).content\n",
    "            except:\n",
    "                print('Connection Problem for url: ', url, '. Skipping it.')\n",
    "                break\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "            links = soup.findAll('a', {'class': 'link_overlay'}, href = True)\n",
    "            for link in links:\n",
    "                try:\n",
    "                    inner_html_content = requests.get(base_url+link['href'], headers = headers).content\n",
    "                except:\n",
    "                    print('Connection Problem for article, url: ', base_url+link['href'])\n",
    "                    continue\n",
    "                inner_soup = BeautifulSoup(inner_html_content, \"lxml\")\n",
    "                articleBody = inner_soup.find('div', {'itemprop': 'articleBody'})\n",
    "                if articleBody:\n",
    "                    headline = inner_soup.find('div', {'class': 'right_title'}).text\n",
    "                    print(\"headline >>> \"+headline)\n",
    "                    infile.write(headline+\"\\n\")\n",
    "                    infile.write(str(articleBody.text) + '\\n\\n')\n",
    "\t\t\t\n",
    "            pagination_div = soup.find('div', {'class': 'pagination'})\n",
    "            url = None\n",
    "            if pagination_div:\n",
    "                next_page_link = pagination_div.find('a', {'class': 'next_page'}, href = True)\n",
    "                if next_page_link:\n",
    "                    url = next_page_link['href']\n",
    "        infile.close()\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_prothom_alo(\"2019-09-02\",\"2019-09-02\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
